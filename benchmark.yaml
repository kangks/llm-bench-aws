Models:
- Name: DeepSeek-R1-Distill-Llama-70B-Q4_0.gguf
  URL: https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-70B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-70B-Q4_0.gguf
  RepoId: bartowski/DeepSeek-R1-Distill-Llama-70B-GGUF
  Filename: DeepSeek-R1-Distill-Llama-70B-Q4_0.gguf
- Name: DeepSeek-R1-Distill-Llama-70B-IQ4_NL.gguf
  URL: https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-70B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-70B-IQ4_NL.gguf
  RepoId: bartowski/DeepSeek-R1-Distill-Llama-70B-GGUF
  Filename: DeepSeek-R1-Distill-Llama-70B-IQ4_NL.gguf
Commands:
  - Prompt: "Building a visually appealing website can be done in ten simple steps:/"
    Tokens: 50        
  - Prompt: "Translate from English to Chinese: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models."
    Tokens: 4096
Instances:
  - Type: r8g.2xlarge
    Has_gpu: 0
    Ami: ami-0e532fbed6ef00604
  - Type: m8g.4xlarge
    Has_gpu: 0
    Ami: ami-0e532fbed6ef00604
  - Type: c8g.8xlarge
    Has_gpu: 0
    Ami: ami-0e532fbed6ef00604
  - Type: g6e.2xlarge
    Has_gpu: 1
    Ami: ami-03ee2c35188fd39a8
